# -*- coding: utf-8 -*-
"""Income2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15NR3gLBZK3k4RpSu85NCDzOOtmnwV1xx

Random Forest and Decision Tree Classifiers
"""

!pip install pyspark

#creating spark session

import pandas as pd
import numpy as np
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('ml-income').getOrCreate()
spark

#importing required libraries

from pyspark.ml.feature import StringIndexer
from pyspark.ml.feature import VectorAssembler
from pyspark.ml import Pipeline
from google.colab import files
uploaded = files.upload()

# loading income dataset and droping rows with missing values

df = spark.read.csv('income.csv', header=True, nullValue='?', ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True, inferSchema=True)

df = df.na.drop()

df.printSchema()
df.show(40)

print(df.dtypes)

# converting categorical feature fields into indexes

categorical_columns = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'citizenship']
indexers = [StringIndexer(inputCol=column, outputCol=column + "_index") for column in categorical_columns]

pipeline = Pipeline(stages=indexers)
df = pipeline.fit(df).transform(df)
df.show(10)

# setting all numerical fields into one feature column

numeric_features = [t[0] for t in df.dtypes if t[1] == 'int']

numeric_columns = ['age', 'workclass_index', 'weight', 'education_index', 'education_years', 'marital_status_index', 'occupation_index', 'relationship_index', 'race_index', 'sex_index', 'capital_gain', 'capital_loss', 'hours_per_week', 'citizenship_index']
assembler = VectorAssembler(inputCols = numeric_columns, outputCol = "features")
df = assembler.transform(df)
df.show(10)

# converting target column into indexes

labels = StringIndexer(inputCol='income_class', outputCol ='income_class_index')
df = labels.fit(df).transform(df)
df.show(10)

print(pd.DataFrame(df.take(100), columns=df.columns))

# splitting dataset into training and testing set

train, test = df.randomSplit([0.7, 0.3])
print('Train Size:' +str(train.count()))
print('Test Size:' +str(test.count()))

from pyspark.ml.classification import RandomForestClassifier

# define Random Forest Classifier, fit model on train set and predict

rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'income_class_index', maxBins=41)
rfModel = rf.fit(train)

predictions = rfModel.transform(test)
predictions.select('age', 'workclass_index', 'weight', 'education_index', 'education_years', 'marital_status_index', 'occupation_index', 'relationship_index', 'race_index', 'sex_index', 'capital_gain', 'capital_loss', 'hours_per_week', 'citizenship_index', 'income_class_index', 'prediction', 'rawPrediction', 'probability').show(25)

from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.classification import DecisionTreeClassifier

# define Decision Tree Classifier, fit model on train set and predict

dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'income_class_index', maxBins=41)
dtModel = dt.fit(train)

dtPredictions = dtModel.transform(test)
dtPredictions.select('age', 'workclass_index', 'weight', 'education_index', 'education_years', 'marital_status_index', 'occupation_index', 'relationship_index', 'race_index', 'sex_index', 'capital_gain', 'capital_loss', 'hours_per_week', 'citizenship_index', 'income_class_index', 'Prediction', 'rawPrediction', 'Probability').show(25)

# define evaluator and calculate accuracies for RT and DT Classifier

evaluator = MulticlassClassificationEvaluator(labelCol = 'income_class_index', predictionCol = 'prediction')

accuracy_rt = evaluator.evaluate(predictions)

accuracy_dt = evaluator.evaluate(dtPredictions)

print('Random Forest Accuracy: %s' % (accuracy_rt))
print('Random Forest Error: %s' % (1.0 - accuracy_rt))

print('Decision Tree Accuracy: %s' % (accuracy_dt))
print('Decision Tree Error: %s' % (1.0 - accuracy_dt))

from pyspark.mllib.evaluation import MulticlassMetrics
from pyspark.sql.types import FloatType
import pyspark.sql.functions as F

# define confusion matrix

preds = predictions.select(['prediction', 'income_class_index']).withColumn('income_class_index', F.col('income_class_index').cast(FloatType()))
preds = preds.select(['prediction', 'income_class_index'])
metrics = MulticlassMetrics(preds.rdd.map(tuple))
print(metrics.confusionMatrix().toArray())

